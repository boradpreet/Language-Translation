{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe0ca72-b0aa-4b8c-8a77-7d5151f442d8",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece4ffe-d349-4e22-aec9-8c96f12d68a5",
   "metadata": {},
   "source": [
    "* Language translation is a complex process that goes far beyond simply replacing words from one language with their equivalents in another. It involves a deep understanding of both the source and target languages, as well as the cultural contexts in which they are used. A skilled translator must consider not only the literal meaning of words but also their connotations, idiomatic expressions, and the overall tone and style of the original text. They must also be aware of the target audience and tailor the translation accordingly, ensuring that the message is not only accurate but also culturally appropriate and easily understood. Whether it's translating a legal document, a literary work, or a website, the goal is to bridge the communication gap and enable people from different linguistic backgrounds to share information, ideas, and experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62133182-b32f-45f9-b8bb-2ee20b22a6e8",
   "metadata": {},
   "source": [
    "### Hugging Face Pretrained Model Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f6351-d31f-46f0-b57c-d998551bc547",
   "metadata": {},
   "source": [
    "* Hugging Face has become a cornerstone of modern Natural Language Processing by providing a vast, accessible repository of pretrained models. These models, such as BERT, GPT, and T5, are foundational for a wide array of NLP tasks, from sentiment analysis and question answering to text generation and machine translation. The power of Hugging Face lies in its \"Transformers\" library, which streamlines the process of downloading and utilizing these complex models. Furthermore, the practice of fine-tuning these pretrained models on specific datasets allows developers to adapt them to unique applications with significantly less training data and computational effort than training from scratch. This democratization of advanced NLP capabilities, along with user-friendly \"pipelines\" for simplified implementation, has positioned Hugging Face as an essential resource for researchers and developers seeking to integrate cutting-edge language processing into their projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602484df-4d65-4746-a551-e327ad477b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7c5f83868946bd852681141deea5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d617ec8b18045ce8cd03a07e6ac1cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc76ede9d2d1431a9cb70422e4f14d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f32d053a34eccbf0b71320941ce98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9293ee1b7dea41729639e7c47912f239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b96f05090d472b8a6d9b97d7ebbc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6c76d83d5e4415ad5d0a4c6538acfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आप कैसे हैं?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_to_hindi(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "    # Decode translation\n",
    "    output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "english_text = \"How are you?\"\n",
    "hindi_translation = translate_to_hindi(english_text)\n",
    "print(hindi_translation)  # Output: आप कैसे हैं?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80bdf54-0490-46d7-b40e-78983bcbbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence in English:  Good Morning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Translation: शुभ रात्रि\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_to_hindi(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "    # Decode translation\n",
    "    output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "# Take user input\n",
    "english_text = input(\"Enter a sentence in English: \")\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_translation = translate_to_hindi(english_text)\n",
    "\n",
    "# Print output\n",
    "print(\"Hindi Translation:\", hindi_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc7497-d0a9-48cf-b259-d3c3966d3b79",
   "metadata": {},
   "source": [
    "### Google Translate Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113f805-66fa-44a6-9871-9ac4c853197e",
   "metadata": {},
   "source": [
    "* Google Translate, while widely known as a web service and app, also offers programmatic access through its Cloud Translation API, which essentially functions as its \"library\" for developers. This API allows for the integration of Google's powerful translation capabilities directly into applications and services. Unlike a traditional library that you download and install, the Cloud Translation API is a cloud-based service, meaning you send requests to Google's servers and receive translated text in response. This allows for real-time translation, language detection, and even glossary customization for specific terminology. The API leverages Google's vast datasets and sophisticated machine learning models, including neural machine translation, to provide accurate and contextually relevant translations across a wide range of languages. Developers can use this \"library\" to build features like multilingual user interfaces, automated document translation, and real-time chat translation, making global communication more accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50e1e75-da40-43cb-abe8-2bf09f8ddc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 0.8/1.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17457 sha256=c909db8276abf3cafc1e369dd1f54e274c340cc0f244a87d7fe8a5af09cba076\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\95\\0f\\04\\b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab 4.2.2 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n",
      "openai 1.61.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94ea11c-db22-4cb6-9c7b-fdb0ad1c5ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence in English:  My name is Preet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Translation: मेरा नाम प्रीत है\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def translate_to_hindi(text):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src='en', dest='hi')\n",
    "    return translation.text\n",
    "\n",
    "# Take user input\n",
    "english_text = input(\"Enter a sentence in English: \")\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_translation = translate_to_hindi(english_text)\n",
    "\n",
    "# Print output\n",
    "print(\"Hindi Translation:\", hindi_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cb7338-1d73-4119-9dc2-5523f4bed8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.6.2)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b785e1-f61c-425b-877d-a9faddbdade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as translator.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "class HindiTranslator:\n",
    "    def translate(self, text):\n",
    "        return GoogleTranslator(source='en', target='hi').translate(text)\n",
    "\n",
    "# Create an instance of the class\n",
    "translator = HindiTranslator()\n",
    "\n",
    "# Save the model using pickle\n",
    "with open(\"translator.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(translator, model_file)\n",
    "\n",
    "print(\"Model saved as translator.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ecd7ed-b852-4c2c-bb1a-0497da21bc86",
   "metadata": {},
   "source": [
    "### Hugging Face pytorch use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a416b1-db52-4f5e-9903-129e4c27a6de",
   "metadata": {},
   "source": [
    "* Hugging Face's PyTorch-based translation capabilities center around their \"Transformers\" library, which provides access to a multitude of pre-trained translation models. These models, often based on architectures like T5 or MarianMT, are readily available for use within PyTorch, a popular deep learning framework. Essentially, Hugging Face provides the tools to leverage these models for translation tasks. You can download a pre-trained model, fine-tune it on your specific translation dataset if needed, and then use it to generate translations. The \"Transformers\" library handles the complexities of loading the model, processing the input text, and generating the output translation. The models are designed to work seamlessly with PyTorch tensors, enabling efficient computation and integration into larger deep learning workflows. This allows developers to build custom translation systems, integrate translation into other applications, and experiment with state-of-the-art translation models, all within the familiar PyTorch environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c06b83-028c-4903-a661-8e5e8dde5ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence in English:  How are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Translation: आप कैसे हैं\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from indic_transliteration.sanscript import transliterate, SCHEMES, IAST, DEVANAGARI\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_to_hindi(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "    # Decode translation\n",
    "    output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Convert Romanized Hindi to proper Devanagari script\n",
    "    hindi_translation = transliterate(output, IAST, DEVANAGARI)  # Explicitly set IAST as input encoding\n",
    "    \n",
    "    return hindi_translation\n",
    "\n",
    "# Take user input\n",
    "english_text = input(\"Enter a sentence in English: \")\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_translation = translate_to_hindi(english_text)\n",
    "\n",
    "# Print output\n",
    "print(\"Hindi Translation:\", hindi_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743b485f-4f1d-414c-a271-0077bd5fef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db91ae-5e98-4e36-93d2-b4acc885a4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
